NLP里的迁移学习

​	利用预训练好的模型来抽取词、句子的特征——如word2vec或语言模型

​	不更新预训练好的模型

​	需要构建新的网络来抓取新任务需要的信息

​		word2vec：用周边几个词训练中心词，看作线性模型，**用来抽特征，当成embedding层。忽略了时许信息**

​		语言模型只看了一个方向，训练的模型不大

# Bert的动机

​	基于微调的NLP模型

​	预训练的模型抽取了足够多的信息

​	新的任务只需要增加一个简单的输出层

# Bert架构

​	只有编码器的transformer

两个版本

​	base：使用12个transformer block（）



​	large