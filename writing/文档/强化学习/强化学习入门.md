# 从代码入门强化学习

## 一、代码效果展示

安装gym

```
pip install gym
```

安装pytorch（可选，暂时不需要）

安装tensorflow（可选，暂时不需要）

```
import gym
import time
env = gym.make( 'CartPole-v0' )			
for i_episode in range( 20 ):              
	state = env.reset()					
	for t in range( 100 ):                
		env.render()                     
		print( state )                  
		action = env.action_space.sample()            
		state, reward, done, info = env.step( action )   
	time.sleep(1)                  		
```

## 二、基于cart-pole(倒立摆)代码的gym环境使用方法

## gym介绍

​		gym是一个强化学习算法的**仿真平台**，内置了很多小游戏的接口，而用户无需过多了解游戏的内部实现，通过简单地调用就可以用来测试和仿真。

​		什么是游戏？我们以贪吃蛇举例：我们可以通过选择一个动作(上下左右，其实保持不动也算是选择了一个动作)控制蛇皮走位，而游戏环境在接收到这个信号之后，会计算下一帧的游戏情况，同时会判断我们是吃到了蛋还是撞到了墙，如果撞墙了那么游戏结束，重置游戏。否则游戏继续，根据选取的下一个动作判断下一帧的情况...

​		上面的每一句话都对应着一个方法，具体如何使用就用代码来说话吧。

​		先创建一个游戏环境，这里选取的游戏环境是cart-pole(倒立摆)，这个游戏咋玩呢，想象一下，你捏着撑衣杆的一段，然后尽可能地让它长时间保持直立。

用gym.make()创建一个游戏环境env，env是gym的核心接口，有了它就意味着你代码里已经有了一个游戏所需要的所有内容了。

```
import gym
env = gym.make( 'CartPole-v0' )   
```

下面相应的介绍env几个方法

## env方法（重点）

```
state = env.reset()     #重启游戏，将环境返回到最初状态
```

```
observation, reward, done, info=env.step(action)     
```

action是选取的动作(同时也是强化学习优化的目标)，在贪吃蛇里是上下左右，在倒立摆中是左右平移。

step(action)表示：根据选取的动作，将游戏推进一个时间步长，这个方法有四个返回值

observation——对游戏环境当前状态的观察(强化学习通过观察来优化动作)：贪吃蛇当前的大小、位置，倒立摆当前倾斜角度、所在位置、摆下落的角速度等等。具体包括了什么信息根据这个游戏环境不同而不同。

reward——这个动作是否取得了奖励：比如说贪吃蛇吃到了一个蛋，或者是倒立摆直立保持了一帧。

done——检测游戏是否已经达到了终止条件：贪吃蛇撞到了墙，或者是倒立摆倾斜超过了一定角度。

info(忽略)：用于调试过程。

```
env.render()：#render渲染，输出当前游戏的一帧，可能是以打印的形式，可能是以图像的形式。
```

## 完整代码及注释

**让小车随机运动，同时不设置游戏结束的条件，让游戏进行一百帧，看看杆子会怎么样运动**

```
import gym
import time
env = gym.make( 'CartPole-v0' )				#创建游戏环境——倒立摆
for i_episode in range( 20 ):              #玩二十局游戏
	state = env.reset()						#每一局游戏的开始需要初始化环境
	for t in range( 100 ):                	#每局游戏最多有一百帧
		env.render()                       #输出每帧的图像
		print( state )                     #打印当前的状态信息
		action = env.action_space.sample()            
		#从游戏的动作空间中随机选择一个动作，这个游戏设定只有两个动作，向左或者向右
		state, reward, done, info = env.step( action )   #根据动作引发下一个环境
	time.sleep(1)                  			#每局游戏结束之后暂停1s
```

**让小车随机运动，设置游戏结束的条件，让游戏进行一百帧**

```
import gym
import time
env = gym.make( 'CartPole-v0' )	
for i_episode in range( 20 ):              
	state = env.reset()
	for t in range( 100 ):              
		env.render()                     
		print( state )                    
		action = env.action_space.sample()            
		state, reward, done, info = env.step( action )   
		if done:                           
		#done表示游戏结束(这里设定为：杆子倾斜超过15°，或者小车的移动超出了范围，或者已经保持直立很久)
			print('Episode #%d finished after %d timesteps' % (i_episode, t)) 
            #打印：“经过x帧之后游戏结束”
			break
	time.sleep(1)  
```

强化学习把学习看作试探评价过程，Agent智能体在环境中选择一个动作，环境接受该动作后状态发生变化，同时产生一个强化信号(奖或惩)反馈给Agent，Agent根据强化信号和环境当前状态再选择下一个动作。

放在贪吃蛇中就是：假设蛇已经走到了靠墙的位置，没有经过训练的它随机采取了一个动作，撞到了墙上，得到了负反馈，那么它在就知道这个动作在这个状态下是不对的，那么下一局游戏中就不会走到墙上了。

![img](https://bkimg.cdn.bcebos.com/pic/3bf33a87e950352a842e0d055343fbf2b2118b6b?x-bce-process=image/resize,m_lfit,w_268,limit_1/format,f_auto)

gym仿真平台提供了便捷的接口，上文中 **[采取了一个动作，撞到了墙上，得到了负反馈]** 用代码来表示就是

```
state, reward, done, info=step(action)
```

我们剩余需要做的就是利用state和reward调整我们选取的动作

三、通过

回想一下我们现有东西和需要完成的目标：

现有：当前的状态state，以及上一个状态采取动作A'之后获得的奖励

需要完成的目标：输出一个最优动作A(这里是向左还是向右)

## 三、通过强化学习算法优化动作取得最大奖励

上文中，小车的动作是从动作空间中随机选取的，可以看到，杆子在随机动作下只能保持直立很短的时间。



状态动作可数，value-based的表格法：sarsa和Qlearning

Q learning理解：

